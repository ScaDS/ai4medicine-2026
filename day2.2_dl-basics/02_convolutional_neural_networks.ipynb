{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3036615",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network Cats vs. Dogs Classifier\n",
    "\n",
    "Goal:\n",
    "- Use the Oxford-IIIT Pet dataset (same dataset source often used in fastai examples)\n",
    "- Convert breed labels into a binary task: cat vs dog\n",
    "- Train a CNN with transfer learning (ResNet18)\n",
    "- Inspect learning curves and discuss underfitting / overfitting\n",
    "- Experiment with learning rates and regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4e6275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb46abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5f62de",
   "metadata": {},
   "source": [
    "### 1) Dataset: Oxford-IIIT Pet (PyTorch / torchvision)\n",
    "\n",
    "We use the same dataset source often seen in fastai tutorials, but load it in PyTorch.\n",
    "The original task is breed classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c3ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"./data\"  # dataset will be downloaded here if not present\n",
    "\n",
    "# We load the full set once without transforms (for splitting / label mapping)\n",
    "base_ds = OxfordIIITPet(root=DATA_ROOT, split=\"trainval\", target_types=\"category\", download=True)\n",
    "\n",
    "print(\"Total samples:\", len(base_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7502c8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Breed names in torchvision Oxford-IIIT Pet (37 classes)\n",
    "# Convention in this dataset: Cat breeds start with uppercase letters, dog breeds start with lowercase letters\n",
    "class_names = base_ds.classes\n",
    "print(\"Number of breed classes:\", len(class_names))\n",
    "print(\"First 10 classes:\", class_names[:10])\n",
    "\n",
    "cat_breeds = [c for c in class_names if c[0].isupper()]\n",
    "dog_breeds = [c for c in class_names if c[0].islower()]\n",
    "\n",
    "print(\"Cat breeds:\", len(cat_breeds))\n",
    "print(\"Dog breeds:\", len(dog_breeds))\n",
    "print(\"Example cat breeds:\", cat_breeds[:5])\n",
    "print(\"Example dog breeds:\", dog_breeds[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a8794e",
   "metadata": {},
   "source": [
    "### 2) Transforms (augmentation + normalization)\n",
    "\n",
    "We use ImageNet normalization because ResNet18 pretrained weights expect it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0581884",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std =[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "eval_tfms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std =[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fac4de0",
   "metadata": {},
   "source": [
    "### 3) Create a binary cats-vs-dogs wrapper dataset\n",
    "\n",
    "`OxfordIIITPet` returns breed category labels (0..36).\n",
    "We map them to:\n",
    "- 0 = cat\n",
    "- 1 = dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef42e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OxfordPetCatsDogsBinary(Dataset):\n",
    "    def __init__(self, root, split=\"trainval\", transform=None, download=False):\n",
    "        self.ds = OxfordIIITPet(\n",
    "            root=root,\n",
    "            split=split,\n",
    "            target_types=\"category\",\n",
    "            transform=None,   # apply transform manually in __getitem__\n",
    "            download=download\n",
    "        )\n",
    "        self.transform = transform\n",
    "        self.classes_breeds = self.ds.classes  # breed names\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, breed_idx = self.ds[idx]  # breed_idx is int in [0, 36]\n",
    "\n",
    "        breed_name = self.classes_breeds[breed_idx]\n",
    "        # Oxford-IIIT Pet naming convention:\n",
    "        # Cat breeds start with uppercase letters, dog breeds with lowercase letters\n",
    "        binary_label = 0 if breed_name[0].isupper() else 1  # 0=cat, 1=dog\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, binary_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594b348e",
   "metadata": {},
   "source": [
    "### 4) Train/validation split and transform assignment\n",
    "\n",
    "We split `trainval` into train + validation, then assign transforms separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b6e1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dataset object and then split into train/val/test\n",
    "full_trainval_ds = OxfordPetCatsDogsBinary(root=DATA_ROOT, split=\"trainval\", transform=None, download=True)\n",
    "test_ds_full = OxfordPetCatsDogsBinary(root=DATA_ROOT, split=\"test\", transform=None, download=True)\n",
    "\n",
    "print(\"Trainval samples:\", len(full_trainval_ds))\n",
    "print(\"Test samples:\", len(test_ds_full))\n",
    "\n",
    "n_total = len(full_trainval_ds)\n",
    "n_train = int(0.8 * n_total)\n",
    "n_val = n_total - n_train\n",
    "\n",
    "train_subset, val_subset = random_split(\n",
    "    full_trainval_ds,\n",
    "    [n_train, n_val],\n",
    "    generator=torch.Generator().manual_seed(SEED)\n",
    ")\n",
    "\n",
    "print(\"Train subset:\", len(train_subset))\n",
    "print(\"Val subset:\", len(val_subset))\n",
    "\n",
    "# %%\n",
    "# Helper wrapper to apply different transforms to subsets created by random_split\n",
    "class TransformSubset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.subset[idx]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# IMPORTANT:\n",
    "# `full_trainval_ds` currently returns PIL images because transform=None\n",
    "train_ds = TransformSubset(train_subset, transform=train_tfms)\n",
    "val_ds = TransformSubset(val_subset, transform=eval_tfms)\n",
    "test_ds = OxfordPetCatsDogsBinary(root=DATA_ROOT, split=\"test\", transform=eval_tfms, download=True)\n",
    "\n",
    "# %%\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "binary_class_names = [\"cat\", \"dog\"]\n",
    "\n",
    "print(\"Train batches:\", len(train_loader))\n",
    "print(\"Val batches:\", len(val_loader))\n",
    "print(\"Test batches:\", len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d014fe",
   "metadata": {},
   "source": [
    "### 5) Visualize a few images (sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbfcb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(img_tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    return (img_tensor * std + mean).clamp(0, 1)\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(min(8, len(images))):\n",
    "    plt.subplot(2, 4, i + 1)\n",
    "    img = denormalize(images[i]).permute(1, 2, 0).numpy()\n",
    "    plt.imshow(img)\n",
    "    plt.title(binary_class_names[labels[i].item()])\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cad5a8",
   "metadata": {},
   "source": [
    "### 6) Model definition (separate class)\n",
    "\n",
    "We use transfer learning with ResNet18 and replace the final layer for 2 classes.\n",
    "Participants can easily swap the backbone or change dropout / freezing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd8ef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBinaryClassifier(nn.Module):\n",
    "    def __init__(self, backbone=\"resnet18\", pretrained=True, dropout=0.0, freeze_backbone=False, num_classes=2):\n",
    "        super().__init__()\n",
    "\n",
    "        if backbone == \"resnet18\":\n",
    "            weights = models.ResNet18_Weights.DEFAULT if pretrained else None\n",
    "            self.backbone = models.resnet18(weights=weights)\n",
    "            in_features = self.backbone.fc.in_features\n",
    "        elif backbone == \"resnet34\":\n",
    "            weights = models.ResNet34_Weights.DEFAULT if pretrained else None\n",
    "            self.backbone = models.resnet34(weights=weights)\n",
    "            in_features = self.backbone.fc.in_features\n",
    "        elif backbone == \"resnet50\":\n",
    "            weights = models.ResNet50_Weights.DEFAULT if pretrained else None\n",
    "            self.backbone = models.resnet50(weights=weights)\n",
    "            in_features = self.backbone.fc.in_features\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported backbone. Choose from: resnet18, resnet34, resnet50\")\n",
    "\n",
    "        if freeze_backbone:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        if dropout > 0:\n",
    "            self.backbone.fc = nn.Sequential(\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(in_features, num_classes)\n",
    "            )\n",
    "        else:\n",
    "            self.backbone.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8390ad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNetBinaryClassifier(\n",
    "    backbone=\"resnet18\",   # easy to explain and fast enough for workshop\n",
    "    pretrained=True,\n",
    "    dropout=0.2,\n",
    "    freeze_backbone=False, # try True for feature-extractor mode\n",
    "    num_classes=2\n",
    ").to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e59b57",
   "metadata": {},
   "source": [
    "### 7) Loss, optimizer, scheduler\n",
    "\n",
    "- `CrossEntropyLoss` for 2-class classification\n",
    "- Adam optimizer\n",
    "- optional scheduler to reduce LR on plateaus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34071a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "LEARNING_RATE = 1e-4  # try 1e-5, 1e-4, 1e-3 for the learning-rate experiment\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"min\",\n",
    "    factor=0.5,\n",
    "    patience=2\n",
    ")\n",
    "\n",
    "print(\"Initial LR:\", optimizer.param_groups[0][\"lr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc47fb37",
   "metadata": {},
   "source": [
    "### 8) Training / validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7150da64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, loader, criterion, optimizer=None, device=\"cpu\"):\n",
    "    is_train = optimizer is not None\n",
    "    model.train() if is_train else model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    n_samples = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            if is_train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        batch_size = xb.size(0)\n",
    "        running_loss += loss.item() * batch_size\n",
    "        running_correct += (preds == yb).sum().item()\n",
    "        n_samples += batch_size\n",
    "\n",
    "        all_preds.append(preds.detach().cpu())\n",
    "        all_targets.append(yb.detach().cpu())\n",
    "\n",
    "    epoch_loss = running_loss / n_samples\n",
    "    epoch_acc = running_correct / n_samples\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "    all_targets = torch.cat(all_targets).numpy()\n",
    "\n",
    "    return epoch_loss, epoch_acc, all_preds, all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb77ac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler=None,\n",
    "    device=\"cpu\",\n",
    "    num_epochs=10,\n",
    "    early_stopping_patience=None\n",
    "):\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_acc\": [],\n",
    "        \"lr\": []\n",
    "    }\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss, train_acc, _, _ = run_epoch(model, train_loader, criterion, optimizer=optimizer, device=device)\n",
    "        val_loss, val_acc, _, _ = run_epoch(model, val_loader, criterion, optimizer=None, device=device)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        history[\"lr\"].append(current_lr)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d}/{num_epochs} | \"\n",
    "            f\"train_loss={train_loss:.4f}, train_acc={train_acc:.4f} | \"\n",
    "            f\"val_loss={val_loss:.4f}, val_acc={val_acc:.4f} | \"\n",
    "            f\"lr={current_lr:.2e}\"\n",
    "        )\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if early_stopping_patience is not None and epochs_without_improvement >= early_stopping_patience:\n",
    "            print(f\"Early stopping triggered after {epoch} epochs.\")\n",
    "            break\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Training finished in {elapsed/60:.1f} min\")\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74075656",
   "metadata": {},
   "source": [
    "### 9) Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28ef257",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 8  # try 3, 8, 15, 25 to show under/overfitting behavior\n",
    "\n",
    "model, history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    early_stopping_patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695189b3",
   "metadata": {},
   "source": [
    "### 10) Plot training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf836d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(epochs, history[\"train_loss\"], label=\"Train loss\")\n",
    "plt.plot(epochs, history[\"val_loss\"], label=\"Validation loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss curves\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(epochs, history[\"train_acc\"], label=\"Train accuracy\")\n",
    "plt.plot(epochs, history[\"val_acc\"], label=\"Validation accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy curves\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(epochs, history[\"lr\"], label=\"Learning rate\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"LR\")\n",
    "plt.title(\"Learning rate during training\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69a1134",
   "metadata": {},
   "source": [
    "### 10) Final evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2b70aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc, val_preds, val_targets = run_epoch(\n",
    "    model, val_loader, criterion, optimizer=None, device=device\n",
    ")\n",
    "\n",
    "print(f\"Validation loss: {val_loss:.4f}\")\n",
    "print(f\"Validation accuracy: {val_acc:.4f}\")\n",
    "print()\n",
    "print(classification_report(val_targets, val_preds, target_names=binary_class_names))\n",
    "\n",
    "cm = confusion_matrix(val_targets, val_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=binary_class_names)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "test_loss, test_acc, test_preds, test_targets = run_epoch(\n",
    "    model, test_loader, criterion, optimizer=None, device=device\n",
    ")\n",
    "\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "print()\n",
    "print(classification_report(test_targets, test_preds, target_names=binary_class_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c6c669",
   "metadata": {},
   "source": [
    "### 13) Visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b769756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(model, loader, class_names, device=\"cpu\", n_show=8):\n",
    "    model.eval()\n",
    "    xb, yb = next(iter(loader))\n",
    "    xb = xb.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(xb)\n",
    "        preds = torch.argmax(logits, dim=1).cpu()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(min(n_show, len(xb))):\n",
    "        plt.subplot(2, 4, i + 1)\n",
    "        img = denormalize(xb[i].cpu()).permute(1, 2, 0).numpy()\n",
    "        true_label = class_names[yb[i].item()]\n",
    "        pred_label = class_names[preds[i].item()]\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"T: {true_label}\\nP: {pred_label}\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "predict_batch(model, val_loader, binary_class_names, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc353fdd",
   "metadata": {},
   "source": [
    "### (Optional) Feature extraction from the penultimate layer\n",
    "\n",
    "We extract embeddings from the layer before the final classifier (fc) and visualize them in 2D using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902a5596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_resnet_features(model, loader, device=\"cpu\", max_samples=500):\n",
    "    \"\"\"\n",
    "    Extract penultimate-layer features from a ResNet model.\n",
    "    Returns:\n",
    "        features: np.ndarray of shape (N, D)\n",
    "        labels: np.ndarray of shape (N,)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # We use the backbone up to (but excluding) the final fc layer.\n",
    "    # For ResNet: conv -> ... -> avgpool -> flatten -> fc\n",
    "    feature_extractor = nn.Sequential(*list(model.backbone.children())[:-1]).to(device)\n",
    "    feature_extractor.eval()\n",
    "\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    n_collected = 0\n",
    "\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "\n",
    "        feats = feature_extractor(xb)          # shape: (B, 512, 1, 1) for ResNet18\n",
    "        feats = torch.flatten(feats, 1)        # shape: (B, 512)\n",
    "\n",
    "        all_features.append(feats.cpu())\n",
    "        all_labels.append(yb.cpu())\n",
    "\n",
    "        n_collected += xb.size(0)\n",
    "        if n_collected >= max_samples:\n",
    "            break\n",
    "\n",
    "    features = torch.cat(all_features, dim=0)[:max_samples].numpy()\n",
    "    labels = torch.cat(all_labels, dim=0)[:max_samples].numpy()\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "features, feat_labels = extract_resnet_features(model, val_loader, device=device, max_samples=400)\n",
    "\n",
    "print(\"Feature matrix shape:\", features.shape)\n",
    "print(\"Labels shape:\", feat_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff9ee10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "features_2d = pca.fit_transform(features)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "for class_idx, class_name in enumerate(binary_class_names):\n",
    "    mask = feat_labels == class_idx\n",
    "    plt.scatter(\n",
    "        features_2d[mask, 0],\n",
    "        features_2d[mask, 1],\n",
    "        label=class_name,\n",
    "        alpha=0.7,\n",
    "        s=20\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"PCA of CNN features (penultimate layer)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Explained variance ratio (PC1 + PC2):\", pca.explained_variance_ratio_.sum().round(3))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
